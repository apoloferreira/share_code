{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import udf\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row, Column\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.pandas.typedef import as_spark_type\n",
    "\n",
    "from pyspark.ml.feature import Bucketizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "from enum import Enum, auto\n",
    "from itertools import chain\n",
    "from decimal import Decimal\n",
    "from datetime import datetime, date\n",
    "import contextlib\n",
    "import sys\n",
    "import math\n",
    "import io\n",
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/24 11:54:56 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "24/08/24 11:54:56 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/24 11:54:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Testes\")\n",
    "    .config('spark.sql.adaptive.enabled', 'true')\n",
    "    .config('spark.sql.adaptive.optimizerEnabled', 'true')\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample DataFrames for expected (reference) and actual distributions\n",
    "expected_data = [(1,), (2,), (2,), (3,), (4,), (4,), (5,), (5,), (6,), (7,)]\n",
    "actual_data = [(1,), (2,), (2,), (3,), (4,), (4,), (5,), (5,), (6,), (8,)]\n",
    "\n",
    "expected_df = spark.createDataFrame(expected_data, [\"value\"])\n",
    "actual_df = spark.createDataFrame(actual_data, [\"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|    1|\n",
      "|    2|\n",
      "|    2|\n",
      "|    3|\n",
      "|    4|\n",
      "|    4|\n",
      "|    5|\n",
      "|    5|\n",
      "|    6|\n",
      "|    8|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CDF for the expected distribution\n",
    "expected_window = Window.orderBy(F.col(\"value\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "expected_cdf = expected_df.withColumn(\"expected_cdf\", F.cume_dist().over(expected_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate CDF for the actual distribution\n",
    "actual_window = Window.orderBy(F.col(\"value\")).rangeBetween(Window.unboundedPreceding, 0)\n",
    "actual_cdf = actual_df.withColumn(\"actual_cdf\", F.cume_dist().over(actual_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "|value|actual_cdf|\n",
      "+-----+----------+\n",
      "|    1|       0.1|\n",
      "|    2|       0.3|\n",
      "|    2|       0.3|\n",
      "|    3|       0.4|\n",
      "|    4|       0.6|\n",
      "|    4|       0.6|\n",
      "|    5|       0.8|\n",
      "|    5|       0.8|\n",
      "|    6|       0.9|\n",
      "|    8|       1.0|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "actual_cdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two CDFs by value\n",
    "joined_cdf = expected_cdf.alias(\"e\")\\\n",
    "    .join(other=actual_cdf.alias(\"a\"), on=F.col(\"e.value\") == F.col(\"a.value\"), how=\"outer\")\\\n",
    "    .select(\n",
    "        F.col(\"e.value\").alias(\"value\"),\n",
    "        F.col(\"e.expected_cdf\"),\n",
    "        F.col(\"a.actual_cdf\")\n",
    "    ).orderBy(\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+\n",
      "|value|expected_cdf|actual_cdf|\n",
      "+-----+------------+----------+\n",
      "| null|        null|       1.0|\n",
      "|    1|         0.1|       0.1|\n",
      "|    2|         0.3|       0.3|\n",
      "|    2|         0.3|       0.3|\n",
      "|    2|         0.3|       0.3|\n",
      "|    2|         0.3|       0.3|\n",
      "|    3|         0.4|       0.4|\n",
      "|    4|         0.6|       0.6|\n",
      "|    4|         0.6|       0.6|\n",
      "|    4|         0.6|       0.6|\n",
      "|    4|         0.6|       0.6|\n",
      "|    5|         0.8|       0.8|\n",
      "|    5|         0.8|       0.8|\n",
      "|    5|         0.8|       0.8|\n",
      "|    5|         0.8|       0.8|\n",
      "|    6|         0.9|       0.9|\n",
      "|    7|         1.0|      null|\n",
      "+-----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_cdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missing values with the last seen value\n",
    "# joined_cdf = joined_cdf.fillna(method='ffill')\n",
    "\n",
    "# Replace any remaining nulls with 0 (in case the first value was null)\n",
    "joined_cdf = joined_cdf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+\n",
      "|value|expected_cdf|actual_cdf|\n",
      "+-----+------------+----------+\n",
      "|    0|         0.0|       1.0|\n",
      "|    1|         0.1|       0.1|\n",
      "|    2|         0.3|       0.3|\n",
      "|    2|         0.3|       0.3|\n",
      "|    2|         0.3|       0.3|\n",
      "|    2|         0.3|       0.3|\n",
      "|    3|         0.4|       0.4|\n",
      "|    4|         0.6|       0.6|\n",
      "|    4|         0.6|       0.6|\n",
      "|    4|         0.6|       0.6|\n",
      "|    4|         0.6|       0.6|\n",
      "|    5|         0.8|       0.8|\n",
      "|    5|         0.8|       0.8|\n",
      "|    5|         0.8|       0.8|\n",
      "|    5|         0.8|       0.8|\n",
      "|    6|         0.9|       0.9|\n",
      "|    7|         1.0|       0.0|\n",
      "+-----+------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_cdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute difference between the expected and actual CDFs\n",
    "joined_cdf = joined_cdf.withColumn(\"ks_difference\", F.abs(F.col(\"expected_cdf\") - F.col(\"actual_cdf\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+----------+-------------+\n",
      "|value|expected_cdf|actual_cdf|ks_difference|\n",
      "+-----+------------+----------+-------------+\n",
      "|    0|         0.0|       1.0|          1.0|\n",
      "|    1|         0.1|       0.1|          0.0|\n",
      "|    2|         0.3|       0.3|          0.0|\n",
      "|    2|         0.3|       0.3|          0.0|\n",
      "|    2|         0.3|       0.3|          0.0|\n",
      "|    2|         0.3|       0.3|          0.0|\n",
      "|    3|         0.4|       0.4|          0.0|\n",
      "|    4|         0.6|       0.6|          0.0|\n",
      "|    4|         0.6|       0.6|          0.0|\n",
      "|    4|         0.6|       0.6|          0.0|\n",
      "|    4|         0.6|       0.6|          0.0|\n",
      "|    5|         0.8|       0.8|          0.0|\n",
      "|    5|         0.8|       0.8|          0.0|\n",
      "|    5|         0.8|       0.8|          0.0|\n",
      "|    5|         0.8|       0.8|          0.0|\n",
      "|    6|         0.9|       0.9|          0.0|\n",
      "|    7|         1.0|       0.0|          1.0|\n",
      "+-----+------------+----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_cdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KS Statistic: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Find the maximum difference, which is the KS statistic\n",
    "ks_statistic = joined_cdf.agg({\"ks_difference\": \"max\"}).collect()[0][0]\n",
    "print(f\"KS Statistic: {ks_statistic}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
