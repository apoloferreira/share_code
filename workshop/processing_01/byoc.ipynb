{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sagemaker Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring your own container"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "https://docs.aws.amazon.com/sagemaker/latest/dg/studio-byoi.html\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/processing-container-run-scripts.html\n",
    "https://docs.aws.amazon.com/sagemaker/latest/dg/build-your-own-processing-container.html\n",
    "https://aws.amazon.com/pt/blogs/machine-learning/using-the-amazon-sagemaker-studio-image-build-cli-to-build-container-images-from-your-studio-notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q \\\n",
    "#     sagemaker \\\n",
    "#     sagemaker-studio-image-build \\\n",
    "#     sagemaker-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !sh build_and_push_studio.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import IPython\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker import image_uris\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.processing import Processor, ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.workflow.parameters import Parameter\n",
    "from sagemaker.workflow.execution_variables import ExecutionVariable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !{sys.executable} -m pip install -U sagemaker smdebug sagemaker-studio-image-build\n",
    "# IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMAGE: 891377318910.dkr.ecr.us-east-1.amazonaws.com/custom-image-teste1:latest\n"
     ]
    }
   ],
   "source": [
    "ACCOUNT_ID = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "REGION = boto3.session.Session().region_name\n",
    "URI_SUFFIX = \"amazonaws.com\"\n",
    "ECR_REPOSITORY = \"custom-image-teste1\"\n",
    "TAG = \"latest\"\n",
    "\n",
    "byoc_image_uri = \"{}.dkr.ecr.{}.{}/{}:{}\".format(ACCOUNT_ID, REGION, URI_SUFFIX, ECR_REPOSITORY, TAG)\n",
    "print(f\"IMAGE: {byoc_image_uri}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_input_path = \"s3://bucket/prefix/\"\n",
    "base_job_name = \"custom-container-test-job\"\n",
    "instance_count = 1\n",
    "instance_type = \"ml.c5.xlarge\"\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use your own Processing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor = ScriptProcessor(\n",
    "    command=[\"python3\"],\n",
    "    image_uri=byoc_image_uri,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./preprocessing.py\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"[%(asctime)s] %(levelname)s %(name)s %(filename)s %(funcName)s %(lineno)d: %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")\n",
    "\n",
    "def _parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # Data, model, and output directories\n",
    "    # model_dir is always passed in from SageMaker. By default this is a S3 path under the default bucket.\n",
    "    parser.add_argument('--filepath', type=str, default='/opt/ml/processing/input/')\n",
    "    parser.add_argument('--filename', type=str, default='bank-additional-full.csv')\n",
    "    parser.add_argument('--outputpath', type=str, default='/opt/ml/processing/output/')\n",
    "    parser.add_argument('--logger_level', type=str, default='INFO')\n",
    "    return parser.parse_known_args()\n",
    "\n",
    "\n",
    "def process_data(df_data):\n",
    "    # Indicator variable to capture when pdays takes a value of 999\n",
    "    df_data[\"no_previous_contact\"] = np.where(df_data[\"pdays\"] == 999, 1, 0)\n",
    "\n",
    "    # Indicator for individuals not actively employed\n",
    "    df_data[\"not_working\"] = np.where(\n",
    "        np.in1d(df_data[\"job\"], [\"student\", \"retired\", \"unemployed\"]), 1, 0\n",
    "    )\n",
    "\n",
    "    # remove unnecessary data\n",
    "    df_model_data = df_data.drop(\n",
    "        [\"duration\", \"emp.var.rate\", \"cons.price.idx\", \"cons.conf.idx\", \"euribor3m\", \"nr.employed\"],\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    bins = [18, 30, 40, 50, 60, 70, 90]\n",
    "    labels = ['18-29', '30-39', '40-49', '50-59', '60-69', '70-plus']\n",
    "\n",
    "    df_model_data['age_range'] = pd.cut(df_model_data.age, bins, labels=labels, include_lowest=True)\n",
    "    df_model_data = pd.concat([df_model_data, pd.get_dummies(df_model_data['age_range'], prefix='age', dtype=int)], axis=1)\n",
    "    df_model_data.drop('age', axis=1, inplace=True)\n",
    "    df_model_data.drop('age_range', axis=1, inplace=True)\n",
    "\n",
    "    scaled_features = ['pdays', 'previous', 'campaign']\n",
    "    df_model_data[scaled_features] = MinMaxScaler().fit_transform(df_model_data[scaled_features])\n",
    "\n",
    "    df_model_data = pd.get_dummies(df_model_data, dtype=int)  # Convert categorical variables to sets of indicators\n",
    "\n",
    "    # Replace \"y_no\" and \"y_yes\" with a single label column, and bring it to the front:\n",
    "    df_model_data = pd.concat(\n",
    "        [\n",
    "            df_model_data[\"y_yes\"].rename(target_col),\n",
    "            df_model_data.drop([\"y_no\", \"y_yes\"], axis=1),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    \n",
    "    return df_model_data\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Process arguments\n",
    "    args, _ = _parse_args()\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.setLevel(args.logger_level)\n",
    "    target_col = \"y\"\n",
    "    logger.info(\"TESTANDO\")\n",
    "\n",
    "    df_input = pd.read_csv(os.path.join(args.filepath, args.filename), sep=\";\")\n",
    "\n",
    "    # process data\n",
    "    df_model_data = process_data(df_input)\n",
    "\n",
    "    print(f\"Data:{df_model_data.shape}\")\n",
    "\n",
    "    # Save datasets locally\n",
    "    df_model_data.to_csv(os.path.join(args.outputpath, 'base/dataset.csv'), index=False, header=False)\n",
    "  \n",
    "    print(\"## Processing complete. Exiting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = \"sagemaker-us-east-1-891377318910\"\n",
    "s3_prefix = \"workshop_v2\"\n",
    "dataset_raw = \"bank-additional-full.csv\"\n",
    "\n",
    "s3_input_path = f\"s3://{bucket_name}/{s3_prefix}/data/raw/{dataset_raw}\"\n",
    "s3_output_path = f\"s3://{bucket_name}/{s3_prefix}/data/transformed/baseline/dataset.csv\"\n",
    "print(f\"Input file:  {s3_input_path}\")\n",
    "print(f\"Output file: {s3_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {s3_input_path} --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_processor.run(\n",
    "    code=\"preprocessing.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=s3_input_path,\n",
    "            destination=\"/opt/ml/processing/input\",\n",
    "            input_name=\"input_data\",\n",
    "            s3_input_mode=\"File\",\n",
    "            s3_data_distribution_type=\"FullyReplicated\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/output/base\",\n",
    "            destination=s3_output_path,\n",
    "            output_name=\"processed_dataset\",\n",
    "        ),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your own Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Processor(\n",
    "    image_uri=byoc_image_uri,\n",
    "    role=role,\n",
    "    instance_count=instance_count,\n",
    "    instance_type=instance_type\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.run(\n",
    "    job_name=\"job-run-teste\",\n",
    "    arguments=[\"arg\"],\n",
    "    wait=True,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=\"s3://path/to/my/input-data.csv\",\n",
    "            destination=\"/opt/ml/processing/input_data\",\n",
    "            input_name=\"input_data\",\n",
    "            s3_data_type=\"S3Prefix\",                     # Valid options: \"ManifestFile\" or \"S3Prefix\"\n",
    "            s3_input_mode=\"File\",                        # Valid options: \"Pipe\", \"File\" or \"FastFile\"\n",
    "            s3_data_distribution_type=\"FullyReplicated\", # Valid options: \"FullyReplicated\" or \"ShardedByS3Key\"\n",
    "            # s3_compression_type=\"snappy\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=\"/opt/ml/processing/processed_data\",\n",
    "            destination=\"<s3_uri>\",\n",
    "            output_name=\"processed_dataset\",\n",
    "            # s3_upload_mode = \"\",\n",
    "            # feature_store_output = \"\",\n",
    "        )\n",
    "    ],                     \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
