{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "from pyspark.ml.regression import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/29 17:40:05 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "24/06/29 17:40:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/06/29 17:40:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(appName=\"Evidently\")\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(path='../data/dataset.csv', inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "|cement| slag|flyash|water|superplasticizer|coarseaggregate|fineaggregate|age|csMPa|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1040.0|        676.0| 28|79.99|\n",
      "| 540.0|  0.0|   0.0|162.0|             2.5|         1055.0|        676.0| 28|61.89|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|270|40.27|\n",
      "| 332.5|142.5|   0.0|228.0|             0.0|          932.0|        594.0|365|41.05|\n",
      "| 198.6|132.4|   0.0|192.0|             0.0|          978.4|        825.5|360| 44.3|\n",
      "+------+-----+------+-----+----------------+---------------+-------------+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1030"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cement: double (nullable = true)\n",
      " |-- slag: double (nullable = true)\n",
      " |-- flyash: double (nullable = true)\n",
      " |-- water: double (nullable = true)\n",
      " |-- superplasticizer: double (nullable = true)\n",
      " |-- coarseaggregate: double (nullable = true)\n",
      " |-- fineaggregate: double (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- csMPa: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de linhas antes de remover valores ausentes: 1030\n",
      "Número de linhas após remover valores ausentes: 1030\n"
     ]
    }
   ],
   "source": [
    "df_nan = df.na.drop()\n",
    "print('Número de linhas antes de remover valores ausentes:', df.count())\n",
    "print('Número de linhas após remover valores ausentes:', df_nan.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = df.randomSplit(weights=[0.7,0.3], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_modulo_prep_dados(df: DataFrame,\n",
    "                           variaveis_entrada: list[str],\n",
    "                           variavel_saida: str,\n",
    "                           tratar_outliers = True,\n",
    "                           padronizar_dados = True):\n",
    "\n",
    "    # Vamos gerar um novo dataframe, renomeando o argumento que representa a variável de saída.\n",
    "    novo_df = df.withColumnRenamed(variavel_saida, 'label')\n",
    "    \n",
    "    # Convertemos a variável alvo para o tipo numérico como float (encoding)\n",
    "    if str(novo_df.schema['label'].dataType) != 'IntegerType':\n",
    "        novo_df = novo_df.withColumn(\"label\", novo_df[\"label\"].cast(T.FloatType()))\n",
    "    \n",
    "    # Listas de controle para as variáveis\n",
    "    variaveis_numericas = []\n",
    "    variaveis_categoricas = []\n",
    "    \n",
    "    # Se tiver variáveis de entrada do tipo string, convertemos para o tipo numérico\n",
    "    for coluna in variaveis_entrada:\n",
    "        \n",
    "        # Verifica se a variável é do tipo string\n",
    "        if str(novo_df.schema[coluna].dataType) == 'StringType':\n",
    "            \n",
    "            # Definimos a variável com um sufixo\n",
    "            novo_nome_coluna = coluna + \"_num\"\n",
    "            \n",
    "            # Adicionamos à lista de variáveis categóricas\n",
    "            variaveis_categoricas.append(novo_nome_coluna)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Se não for variável do tipo string, então é numérica e adicionamos na lista correspondente\n",
    "            variaveis_numericas.append(coluna)\n",
    "            \n",
    "            # Colocamos os dados no dataframe de variáveis indexadas\n",
    "            df_indexed = novo_df\n",
    "            \n",
    "    # Se o dataframe tiver dados do tipo string, aplicamos a indexação\n",
    "    # Verificamos se a lista de variáveis categóricas não está vazia\n",
    "    if len(variaveis_categoricas) != 0:\n",
    "\n",
    "        # Loop pelas colunas\n",
    "        for coluna in novo_df:\n",
    "            \n",
    "            # Se a variável é do tipo string, criamos, treinamos e aplicamos o indexador\n",
    "            if str(novo_df.schema[coluna].dataType) == 'StringType':\n",
    "                \n",
    "                # Cria o indexador\n",
    "                indexer = StringIndexer(inputCol=coluna, outputCol=coluna + \"_num\") \n",
    "                \n",
    "                # Treina e aplica o indexador\n",
    "                df_indexed = indexer.fit(novo_df).transform(novo_df)\n",
    "    else:\n",
    "        # Se não temos mais variáveis categóricas, então colocamos os dados no dataframe de variáveis indexadas\n",
    "        df_indexed = novo_df\n",
    "        \n",
    "    # Se for necessário tratar outliers, faremos isso agora\n",
    "    if tratar_outliers == True:\n",
    "        print(\"\\nAplicando o tratamento de outliers...\")\n",
    "        \n",
    "        # Dicionário\n",
    "        d = {}\n",
    "        \n",
    "        # Dicionário de quartis das variáveis do dataframe indexado (somente variáveis numéricas)\n",
    "        for col in variaveis_numericas: \n",
    "            d[col] = df_indexed.approxQuantile(col, probabilities=[0.01, 0.99], relativeError=0.25) \n",
    "        \n",
    "        # Agora aplicamos transformação dependendo da distribuição de cada variável\n",
    "        for col in variaveis_numericas:\n",
    "            \n",
    "            # Extraímos a assimetria dos dados e usamos isso para tratar os outliers\n",
    "            skew = df_indexed.agg(F.skewness(df_indexed[col])).collect() \n",
    "            skew = skew[0][0]\n",
    "            \n",
    "            # Verificamos a assimetria e então aplicamos:\n",
    "            \n",
    "            # Transformação de log + 1 se a assimetria for positiva\n",
    "            if skew > 1:\n",
    "                indexed = df_indexed.withColumn(col, \n",
    "                    F.log(\n",
    "                        F.when(df[col] < d[col][0], d[col][0])\\\n",
    "                        .when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                        .otherwise(df_indexed[col] ) + 1\n",
    "                    ).alias(col)\n",
    "                )\n",
    "                print(\"\\nA variável \" + col + \" foi tratada para assimetria positiva (direita) com skew =\", skew)\n",
    "            \n",
    "            # Transformação exponencial se a assimetria for negativa\n",
    "            elif skew < -1:\n",
    "                indexed = df_indexed.withColumn(col,\n",
    "                    F.exp(\n",
    "                        F.when(df[col] < d[col][0], d[col][0]).when(df_indexed[col] > d[col][1], d[col][1])\\\n",
    "                        .otherwise(df_indexed[col])\n",
    "                    ).alias(col)\n",
    "                )\n",
    "                print(\"\\nA variável \" + col + \" foi tratada para assimetria negativa (esquerda) com skew =\", skew)\n",
    "                \n",
    "            # Assimetria entre -1 e 1 não precisamos aplicar transformação aos dados\n",
    "\n",
    "    # Vetorização\n",
    "    \n",
    "    # Lista final de atributos\n",
    "    lista_atributos = variaveis_numericas + variaveis_categoricas\n",
    "    \n",
    "    # Cria o vetorizador para os atributos\n",
    "    vetorizador = VectorAssembler(inputCols = lista_atributos, outputCol = 'features')\n",
    "    \n",
    "    # Aplica o vetorizador ao conjunto de dados\n",
    "    dados_vetorizados = vetorizador.transform(df_indexed).select('features', 'label')\n",
    "    \n",
    "    # Se a flag padronizar_dados está como True, então padronizamos os dados colocando-os na mesma escala\n",
    "    if padronizar_dados == True:\n",
    "        print(\"\\nPadronizando o conjunto de dados para o intervalo de 0 a 1...\")\n",
    "        \n",
    "        # Cria o scaler\n",
    "        scaler = MinMaxScaler(inputCol = \"features\", outputCol = \"scaledFeatures\")\n",
    "\n",
    "        # Calcula o sumário de estatísticas e gera o padronizador\n",
    "        global scalerModel\n",
    "        scalerModel = scaler.fit(dados_vetorizados)\n",
    "\n",
    "        # Padroniza as variáveis para o intervalo [min, max]\n",
    "        dados_padronizados = scalerModel.transform(dados_vetorizados)\n",
    "        \n",
    "        # Gera os dados finais\n",
    "        dados_finais = dados_padronizados.select('label', 'scaledFeatures')\n",
    "        \n",
    "        # Renomeia as colunas (requerido pelo Spark)\n",
    "        dados_finais = dados_finais.withColumnRenamed('scaledFeatures', 'features')\n",
    "        \n",
    "        print(\"\\nProcesso Concluído!\")\n",
    "\n",
    "    # Se a flag está como False, então não padronizamos os dados\n",
    "    else:\n",
    "        print(\"\\nOs dados não serão padronizados pois a flag padronizar_dados está com o valor False.\")\n",
    "        dados_finais = dados_vetorizados\n",
    "    \n",
    "    return dados_finais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "variaveis_entrada = df.columns[:-1]\n",
    "variavel_saida = df.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aplicando o tratamento de outliers...\n",
      "\n",
      "A variável age foi tratada para assimetria positiva (direita) com skew = 3.2644145354168086\n",
      "\n",
      "Padronizando o conjunto de dados para o intervalo de 0 a 1...\n",
      "\n",
      "Processo Concluído!\n"
     ]
    }
   ],
   "source": [
    "df_final = func_modulo_prep_dados(df, variaveis_entrada, variavel_saida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|79.99|[1.0,0.0,0.0,0.32...|\n",
      "|61.89|[1.0,0.0,0.0,0.32...|\n",
      "|40.27|[0.52625570776255...|\n",
      "|41.05|[0.52625570776255...|\n",
      "| 44.3|[0.22054794520547...|\n",
      "|47.03|[0.37442922374429...|\n",
      "| 43.7|[0.63470319634703...|\n",
      "|36.45|[0.63470319634703...|\n",
      "|45.85|[0.37442922374429...|\n",
      "|39.29|(8,[0,3,5,7],[0.8...|\n",
      "|38.07|[0.22054794520547...|\n",
      "|28.02|[0.22054794520547...|\n",
      "|43.01|[0.74315068493150...|\n",
      "|42.33|[0.20091324200913...|\n",
      "|47.81|[0.46118721461187...|\n",
      "|52.91|[0.63470319634703...|\n",
      "|39.36|[0.08584474885844...|\n",
      "|56.14|[0.54794520547945...|\n",
      "|40.56|[0.63470319634703...|\n",
      "|42.62|(8,[0,3,5,7],[0.8...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
