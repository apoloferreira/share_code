{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from types import SimpleNamespace\n",
    "from typing import Any, Union, Callable, NamedTuple\n",
    "from functools import wraps\n",
    "from enum import Enum, auto\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from abc import ABC, ABCMeta, abstractmethod\n",
    "from sklearn import datasets\n",
    "from pydantic import (\n",
    "    BaseModel,\n",
    "    ValidationError,\n",
    "    field_validator,\n",
    "    field_serializer,\n",
    "    model_validator,\n",
    "    computed_field,\n",
    "    ValidatorFunctionWrapHandler,\n",
    "    ValidationInfo,\n",
    "    Field,\n",
    "    ConfigDict,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.context import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import udf\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import Row, Column\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.pandas.typedef import as_spark_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.iteritems = pd.DataFrame.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: park.sql.execution.arrow.pyspark.fallback.enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/18 09:06:49 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "24/08/18 09:06:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/08/18 09:06:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Testes\")\n",
    "    .config('spark.sql.adaptive.enabled', 'true')\n",
    "    .config('spark.sql.adaptive.optimizerEnabled', 'true')\n",
    "    .config('spark.sql.execution.arrow.enabled', 'true')\n",
    "    .config('spark.sql.execution.arrow.pyspark.enabled', 'true')\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.legacy.timeParserPolicy\", \"CORRECTED\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", \"true\")\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100000\")\n",
    "    .config(\"park.sql.execution.arrow.pyspark.fallback.enabled\", \"false\")\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = datasets.load_iris(as_frame=True) # classification\n",
    "df_iris = iris_data.frame\n",
    "\n",
    "# bcancer_data = datasets.load_breast_cancer(as_frame=True) # classification\n",
    "# df_bcancer = bcancer_data.frame\n",
    "\n",
    "# diabetes_data = datasets.load_diabetes(as_frame=True) # regression\n",
    "# df_diabetes = diabetes_data.frame\n",
    "\n",
    "# wine_data = datasets.load_wine(as_frame=True) # classification\n",
    "# df_wine = wine_data.frame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp = df_iris.rename({\n",
    "    \"sepal length (cm)\": \"sepal_length\",\n",
    "    \"sepal width (cm)\": \"sepal_width\",\n",
    "    \"petal length (cm)\": \"petal_length\",\n",
    "    \"petal width (cm)\": \"petal_width\",\n",
    "}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0           5.1          3.5           1.4          0.2       0\n",
       "1           4.9          3.0           1.4          0.2       0\n",
       "2           4.7          3.2           1.3          0.2       0\n",
       "3           4.6          3.1           1.5          0.2       0\n",
       "4           5.0          3.6           1.4          0.2       0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/39109045/numpy-where-with-multiple-conditions\n",
    "\n",
    "def energy_class(x: float):\n",
    "    if x > 6:\n",
    "        return 'high'\n",
    "    elif x > 5:\n",
    "        return 'medium'\n",
    "    else:\n",
    "        return 'low'\n",
    "\n",
    "\n",
    "dfp['tipo'] = pd.cut(dfp['sepal_length'], bins=[0, 5, 6, np.inf], labels=['low', 'medium', 'high'])\n",
    "# dfp['tipo'] = np.where(dfp['sepal_length'] > 7, 'high', np.where(dfp['sepal_length'] > 5, 'medium', 'low'))\n",
    "# dfp['tipo'] = dfp['sepal_length'].apply(energy_class)\n",
    "# dfp['tipo'] = np.select([dfp['sepal_length'] > 7, dfp['sepal_length'] > 5], ['high', 'medium'], default='low')\n",
    "# dfp['tipo'] = np.vectorize(lambda x: 'high' if x > 5 else ('medium' if x > 3 else 'low'))(dfp['sepal_length'])\n",
    "# dfp['tipo'] = dfp['sepal_length'].apply(lambda x: 'high' if x > 6 else ('medium' if x > 5 else 'low'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tipo\n",
       "high      61\n",
       "medium    57\n",
       "low       32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp['tipo'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def suit():\n",
    "    return choice(('Spade', 'Heart', 'Diamond', 'Club'))\n",
    "\n",
    "dfp['suit'] = [suit() for _ in range(len(dfp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfp['id'] = [i for i, _ in enumerate(range(len(dfp)), start=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing\n",
    "dfp.iloc[3, 6] = None\n",
    "dfp.iloc[9, 6] = None\n",
    "\n",
    "# Duplicates\n",
    "dfp.iloc[1, 7] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "      <th>tipo</th>\n",
       "      <th>suit</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "      <td>Diamond</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>Spade</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>Diamond</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>Diamond</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0</td>\n",
       "      <td>medium</td>\n",
       "      <td>Club</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>Spade</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>Club</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>Spade</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0</td>\n",
       "      <td>low</td>\n",
       "      <td>None</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal_length  sepal_width  petal_length  petal_width  target    tipo  \\\n",
       "0           5.1          3.5           1.4          0.2       0  medium   \n",
       "1           4.9          3.0           1.4          0.2       0     low   \n",
       "2           4.7          3.2           1.3          0.2       0     low   \n",
       "3           4.6          3.1           1.5          0.2       0     low   \n",
       "4           5.0          3.6           1.4          0.2       0     low   \n",
       "5           5.4          3.9           1.7          0.4       0  medium   \n",
       "6           4.6          3.4           1.4          0.3       0     low   \n",
       "7           5.0          3.4           1.5          0.2       0     low   \n",
       "8           4.4          2.9           1.4          0.2       0     low   \n",
       "9           4.9          3.1           1.5          0.1       0     low   \n",
       "\n",
       "      suit  id  \n",
       "0  Diamond   1  \n",
       "1    Spade   1  \n",
       "2  Diamond   3  \n",
       "3     None   4  \n",
       "4  Diamond   5  \n",
       "5     Club   6  \n",
       "6    Spade   7  \n",
       "7     Club   8  \n",
       "8    Spade   9  \n",
       "9     None  10  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfp.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(dfp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendEnum(Enum):\n",
    "\n",
    "    def _generate_next_value_(name: str, start, count, last_values):\n",
    "        return name.lower()\n",
    "    \n",
    "    @classmethod\n",
    "    def __getitem__(cls, name: str):\n",
    "        return cls(name.lower())\n",
    "    \n",
    "    @classmethod\n",
    "    def to_dict(cls):\n",
    "        return {e.name: e.value for e in cls}\n",
    "    \n",
    "    @classmethod\n",
    "    def keys(cls):\n",
    "        return cls._member_names_\n",
    "    \n",
    "    @classmethod\n",
    "    def values(cls):\n",
    "        return list(map(lambda c: c.value, cls))\n",
    "    \n",
    "    def __str__(self):\n",
    "        return str(self.value)\n",
    "    \n",
    "    def __repr__(self) -> str:\n",
    "        return str(self.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricType(ExtendEnum):\n",
    "    COLUMN = \"COLUMN\"\n",
    "    TABLE = \"TABLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricName(ExtendEnum):\n",
    "    MISSING = auto()\n",
    "    VOLUMETRY = auto()\n",
    "    DUPLICITY = auto()\n",
    "    MEAN = auto()\n",
    "    TESTE = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_value(func: Callable) -> Callable:\n",
    "    @wraps\n",
    "    def wrapper(*args, **kwargs) -> Any:\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teste(*args, **kwargs):\n",
    "    print(f\"{args = }\")\n",
    "    new_args = list(map(lambda x: x.replace('a', 'A'), args))\n",
    "    print(f\"{new_args = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args = ('a', 'b')\n",
      "new_args = ['A', 'b']\n"
     ]
    }
   ],
   "source": [
    "teste(\"a\", \"b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_value(replace_map: dict):\n",
    "    def decorator(func: Callable):\n",
    "        @wraps\n",
    "        def wrapper(*args, **kwargs) -> Any:\n",
    "            new_args = []\n",
    "            new_kwargs = {}\n",
    "\n",
    "            for item in args:\n",
    "                if item in replace_map.keys():\n",
    "                    new_args.append(replace_map[item])\n",
    "                else:\n",
    "                     new_args.append(item)\n",
    "            \n",
    "            for key, value in kwargs.items():\n",
    "                if key in replace_map.keys():\n",
    "                    new_kwargs[key] = replace_map[key]\n",
    "                else:\n",
    "                    new_kwargs[key] = value\n",
    "\n",
    "            # if set(replace_map.keys()).intersection(args):\n",
    "            #     args = list(map(lambda x: x.replace('_key', replace_map.get('_key')), args))\n",
    "            # if set(replace_map.keys()).intersection(kwargs.keys()):\n",
    "            #     kwargs[param_name] = new_value\n",
    "            return func(*new_args, **new_kwargs)\n",
    "        return wrapper\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricMetaClass(type):\n",
    "\n",
    "    def __new__(cls, *args, **kwargs):\n",
    "        return super().__new__(cls, *args, **kwargs)\n",
    "    \n",
    "    def __call__(cls, df: DataFrame, *args, **kwargs):\n",
    "        print(f\"{type(df) = }\")\n",
    "        print(f\"{args = }\")\n",
    "        print(f\"{kwargs = }\")\n",
    "        return cls.evaluate(df, *args, **kwargs)\n",
    "    \n",
    "    @abstractmethod\n",
    "    def evaluate(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombineMeta(ABCMeta, MetricMetaClass):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricBase(ABC):\n",
    "\n",
    "    spark_session = SparkSession.getActiveSession()\n",
    "\n",
    "    @classmethod\n",
    "    def all_metrics(cls) -> list:\n",
    "        return list(set([subclass.name for subclass in cls.__subclasses__()]))\n",
    "\n",
    "    @classmethod\n",
    "    def metrics_by_type(cls, metric_type: MetricType) -> list:\n",
    "        metrics = []\n",
    "        for subclass in cls.__subclasses__():\n",
    "            if metric_type == subclass.type:\n",
    "                metrics.append(subclass.name)\n",
    "        return metrics\n",
    "\n",
    "    @classmethod\n",
    "    def get_metric(cls, metric_name: str) -> 'MetricBase':\n",
    "        for subclass in cls.__subclasses__():\n",
    "            if metric_name == subclass.name:\n",
    "                return subclass\n",
    "\n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def name(self) -> str:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def calculate(self, df: DataFrame, col: str | list) -> Any:\n",
    "        \"Metodo executa metrica\"\n",
    "    \n",
    "    @staticmethod\n",
    "    @abstractmethod\n",
    "    def evaluate(self) -> Any:\n",
    "        \"Metodo executa metrica\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnMissing(MetricBase, metaclass=CombineMeta):\n",
    "    \n",
    "    name: MetricName = MetricName.MISSING.value\n",
    "    type: MetricType = MetricType.COLUMN\n",
    "    schema: T.StructField = T.StructField(name, T.FloatType(), True)\n",
    "    \n",
    "    @classmethod\n",
    "    def calculate(cls):\n",
    "        print(type(cls.spark))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(df: DataFrame, col: str) -> int:\n",
    "        missing = df.select(col).where(F.col(col).isNull()).count()\n",
    "        return missing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TableVolumetry(MetricBase, metaclass=CombineMeta):\n",
    "    \n",
    "    name: MetricName = MetricName.VOLUMETRY.value\n",
    "    type: MetricType = MetricType.TABLE\n",
    "    schema: T.StructField = T.StructField(name, T.FloatType(), True)\n",
    "    \n",
    "    @classmethod\n",
    "    def calculate(cls):\n",
    "        print(type(cls.spark))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(df: DataFrame, *args, **kwargs) -> int:\n",
    "        count = df.count()\n",
    "        return count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnDuplicity(MetricBase, metaclass=CombineMeta):\n",
    "    \n",
    "    name: MetricName = MetricName.DUPLICITY.value\n",
    "    type: MetricType = MetricType.COLUMN\n",
    "    schema: T.StructField = T.StructField(name, T.FloatType(), True)\n",
    "    \n",
    "    @classmethod\n",
    "    def calculate(cls):\n",
    "        print(type(cls.spark))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(df: DataFrame, col: Union[str, list]) -> int:\n",
    "        print(f\"{col = }\")\n",
    "        if isinstance(col, str):\n",
    "            col = col = [col]\n",
    "        distinct = df.select(col).dropDuplicates(subset=col).count()\n",
    "        total = df.count()\n",
    "        return total - distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnTeste(MetricBase, metaclass=CombineMeta):\n",
    "    \n",
    "    name: MetricName = MetricName.TESTE.value\n",
    "    type: MetricType = MetricType.COLUMN\n",
    "    schema: T.StructField = T.StructField(name, T.FloatType(), True)\n",
    "    \n",
    "    @classmethod\n",
    "    def calculate(cls):\n",
    "        print(type(cls.spark))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(df: DataFrame, *args, **kwargs) -> int:\n",
    "        ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnMean(MetricBase, metaclass=CombineMeta):\n",
    "    \n",
    "    name: MetricName = MetricName.MEAN.value\n",
    "    type: MetricType = MetricType.COLUMN\n",
    "    schema: T.StructField = T.StructField(name, T.FloatType(), True)\n",
    "    \n",
    "    @classmethod\n",
    "    def calculate(cls):\n",
    "        print(type(cls.spark))\n",
    "\n",
    "    @staticmethod\n",
    "    def evaluate(df: DataFrame, col: str) -> int:\n",
    "        mean = df.select(F.mean(col)).first()[0]\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['mean', 'duplicity', 'volumetry', 'missing', 'teste']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_metrics = MetricBase.all_metrics()\n",
    "all_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"mrm.yaml\"\n",
    "\n",
    "with open(file_path) as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reference': {'database': 'workspace_db',\n",
       "  'table': 'tb_spec_dataset',\n",
       "  'train_data': {'start': 202001, 'end': 202212}},\n",
       " 'metrics': {'table': ['volumetry'],\n",
       "  'keys': {'names': ['id'],\n",
       "   '_keys': ['duplicity'],\n",
       "   'individual': {'id': ['missing']}},\n",
       "  'features': {'numerical': {'sepal_length': ['mean', 'missing'],\n",
       "    'sepal_width': ['mean'],\n",
       "    'petal_length': ['mean']},\n",
       "   'categorical': {'tipo': ['missing'], 'suit': ['missing']}},\n",
       "  'target': {'target': ['missing']}}}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def key_columns(config: dict) -> dict:\n",
    "    return {\"_keys\": config['keys']['names']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_keys': ['id']}\n"
     ]
    }
   ],
   "source": [
    "map_key_columns = key_columns(config['metrics'])\n",
    "print(map_key_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"_keyss\" in map_key_columns.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def columns_classification(config: dict):\n",
    "    response = {}\n",
    "    temp = {\n",
    "        # \"chaves\": config['keys']['names'],\n",
    "        \"chave\": [\"_keys\"],\n",
    "        \"numerica\": list(config['features']['numerical'].keys()),\n",
    "        \"categorical\": list(config['features']['categorical'].keys()),\n",
    "        \"target\": list(config['target'].keys())\n",
    "    }\n",
    "    for key, values in temp.items():\n",
    "        if isinstance(values, list):\n",
    "            for value in values:\n",
    "                response[value] = key\n",
    "        else:\n",
    "            response[values] = key\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_keys': 'chave',\n",
       " 'sepal_length': 'numerica',\n",
       " 'sepal_width': 'numerica',\n",
       " 'petal_length': 'numerica',\n",
       " 'tipo': 'categorical',\n",
       " 'suit': 'categorical',\n",
       " 'target': 'target'}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_classification = columns_classification(config['metrics'])\n",
    "cols_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_metrics = MetricBase.metrics_by_type(MetricType.COLUMN)\n",
    "table_metrics = MetricBase.metrics_by_type(MetricType.TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dict(config: dict, condition: Callable, output: dict | None = None):\n",
    "    if output is None:\n",
    "        output: dict[str, list] = {}\n",
    "    for key, value in config.items():\n",
    "        if condition(key):\n",
    "            if isinstance(value, dict):\n",
    "                process_dict(value, condition, output)\n",
    "            if isinstance(value, list):\n",
    "                for item in value:\n",
    "                    if key not in output:\n",
    "                        output[key] = []\n",
    "                    output[key].append(item)\n",
    "    return output\n",
    "\n",
    "\n",
    "def process_config_metrics(metric_config: dict, type: MetricType) -> dict:\n",
    "    if type == MetricType.COLUMN:\n",
    "        condition = lambda x: x not in {\"table\", \"names\"}\n",
    "    else:\n",
    "        condition = lambda x: x == \"table\"\n",
    "\n",
    "    metrics_output: dict[str, list[str]] = process_dict(metric_config, condition)\n",
    "    return {key: list(set(value)) for key, value in metrics_output.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_column_map = process_config_metrics(config['metrics'], MetricType.COLUMN)\n",
    "metrics_table_map = process_config_metrics(config['metrics'], MetricType.TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_map = metrics_column_map | {\"_table\": metrics_table_map[\"table\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_keys': ['duplicity'],\n",
      " '_table': ['volumetry'],\n",
      " 'id': ['missing'],\n",
      " 'petal_length': ['mean'],\n",
      " 'sepal_length': ['mean', 'missing'],\n",
      " 'sepal_width': ['mean'],\n",
      " 'suit': ['missing'],\n",
      " 'target': ['missing'],\n",
      " 'tipo': ['missing']}\n"
     ]
    }
   ],
   "source": [
    "pprint(metrics_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_keys': ['duplicity'],\n",
       " 'id': ['missing'],\n",
       " 'sepal_length': ['mean', 'missing'],\n",
       " 'sepal_width': ['mean'],\n",
       " 'petal_length': ['mean'],\n",
       " 'tipo': ['missing'],\n",
       " 'suit': ['missing'],\n",
       " 'target': ['missing'],\n",
       " '_table': ['volumetry']}"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reponse_fileds = [\"atributo\", \"classificacao\", \"tipo\", \"metrica_medida\", \"valor_medido\"]\n",
    "# ResponseTemplate = namedtuple(\"ResponseTemplate\", reponse_fileds)\n",
    "# ResponseTemplate.__new__.__defaults__ = (None, ) * len(reponse_fileds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Measures:\n",
    "\n",
    "    def __init__(self, name: str, classification: str):\n",
    "        self._name = name\n",
    "        self._classification = classification\n",
    "    #     # self._metrics: list[MetricBase] = []\n",
    "    #     for metric in MetricName.values():\n",
    "    #         setattr(self, metric, None)\n",
    "\n",
    "    def set_attribute(self, attr_name, value) -> None:\n",
    "        setattr(self, attr_name, value)\n",
    "        # if hasattr(self, attr_name):\n",
    "        #     setattr(self, attr_name, value)\n",
    "        # else:\n",
    "        #     raise AttributeError(f\"Attribute {attr_name} does not exist.\")\n",
    "\n",
    "    def calculate(self, df: DataFrame) -> list[dict]:\n",
    "        # results = [metric() for metric in self._metrics]\n",
    "        response: list[dict] = []\n",
    "        for value in self.__dict__.values():\n",
    "            if isinstance(value, CombineMeta):\n",
    "                # if _key\n",
    "                metric_result = value(df, self._name)\n",
    "                result = {\n",
    "                    \"atributo\": self._name,\n",
    "                    \"classificacao\": self._classification,\n",
    "                    \"tipo\": value.type.value,\n",
    "                    \"metrica_medida\": value.name,\n",
    "                    \"valor_medido\": metric_result\n",
    "                }\n",
    "                response.append(result)\n",
    "            # elif isinstance(value, str):\n",
    "            #     if value == \"_key\":\n",
    "            #         result = {\n",
    "            #             \"atributo\": self._name,\n",
    "            #             \"classificacao\": self._classification,\n",
    "            #             \"tipo\": value.type.value,\n",
    "            #             \"metrica_medida\": value.name,\n",
    "            #             \"valor_medido\": metric_result\n",
    "            #         }\n",
    "            #     elif value == \"_table\":\n",
    "            #         result = {\n",
    "            #             \"atributo\": self._name,\n",
    "            #             \"classificacao\": self._classification,\n",
    "            #             \"tipo\": value.type.value,\n",
    "            #             \"metrica_medida\": value.name,\n",
    "            #             \"valor_medido\": metric_result\n",
    "            #         } \n",
    "        return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HandlerMeasure:\n",
    "\n",
    "    def __init__(self, metrics_map: dict):\n",
    "        self._metrics_map = self.clean_metrics(metrics_map)\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_metrics(metrics_map: dict) -> dict[str, list]:\n",
    "        return {key: [] for key in metrics_map.keys()}\n",
    "\n",
    "    def add(self, column: str, measure: Measures) -> None:\n",
    "        self._metrics_map[column] = measure\n",
    "\n",
    "    def run(self, df: DataFrame):\n",
    "        # response_template = {metric_name: None for metric_name in all_metrics}\n",
    "        response = {}\n",
    "        for column, measure in self._metrics_map.items():\n",
    "            print(f\"--> {column = }\")\n",
    "            response[column] = measure.calculate(df)\n",
    "            print(\"==================================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_keys': ['duplicity'],\n",
       " 'id': ['missing'],\n",
       " 'sepal_length': ['mean', 'missing'],\n",
       " 'sepal_width': ['mean'],\n",
       " 'petal_length': ['mean'],\n",
       " 'tipo': ['missing'],\n",
       " 'suit': ['missing'],\n",
       " 'target': ['missing'],\n",
       " '_table': ['volumetry']}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_keys': 'chave',\n",
       " 'sepal_length': 'numerica',\n",
       " 'sepal_width': 'numerica',\n",
       " 'petal_length': 'numerica',\n",
       " 'tipo': 'categorical',\n",
       " 'suit': 'categorical',\n",
       " 'target': 'target'}"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler_measures = HandlerMeasure(metrics_map)\n",
    "\n",
    "for column, metrics in metrics_map.items():\n",
    "    measures = Measures(column, cols_classification.get(column))\n",
    "    for metric_name in metrics:       \n",
    "        Metric = MetricBase.get_metric(metric_name)\n",
    "        measures.set_attribute(metric_name, Metric)\n",
    "    handler_measures.add(column, measures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_name': '_keys',\n",
       " '_classification': 'chave',\n",
       " 'duplicity': __main__.ColumnDuplicity}"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "handler_measures._metrics_map['_keys'].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> column = '_keys'\n",
      "type(df) = <class 'pyspark.sql.dataframe.DataFrame'>\n",
      "args = ('_keys',)\n",
      "kwargs = {}\n",
      "col = '_keys'\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "Column '_keys' does not exist. Did you mean one of the following? [id, suit, tipo, target, petal_length, petal_width, sepal_length, sepal_width];\n'Project ['_keys]\n+- LogicalRDD [sepal_length#0, sepal_width#1, petal_length#2, petal_width#3, target#4L, tipo#5, suit#6, id#7L], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[132], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mhandler_measures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[127], line 18\u001b[0m, in \u001b[0;36mHandlerMeasure.run\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m column, measure \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics_map\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 18\u001b[0m     response[column] \u001b[38;5;241m=\u001b[39m \u001b[43mmeasure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcalculate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m==================================================\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[126], line 23\u001b[0m, in \u001b[0;36mMeasures.calculate\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, CombineMeta):\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;66;03m# if _key\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m         metric_result \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m         result \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     25\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124matributo\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name,\n\u001b[1;32m     26\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclassificacao\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_classification,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalor_medido\u001b[39m\u001b[38;5;124m\"\u001b[39m: metric_result\n\u001b[1;32m     30\u001b[0m         }\n\u001b[1;32m     31\u001b[0m         response\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn[105], line 10\u001b[0m, in \u001b[0;36mMetricMetaClass.__call__\u001b[0;34m(cls, df, *args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m= }\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[110], line 16\u001b[0m, in \u001b[0;36mColumnDuplicity.evaluate\u001b[0;34m(df, col)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m     15\u001b[0m     col \u001b[38;5;241m=\u001b[39m col \u001b[38;5;241m=\u001b[39m [col]\n\u001b[0;32m---> 16\u001b[0m distinct \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdropDuplicates(subset\u001b[38;5;241m=\u001b[39mcol)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     17\u001b[0m total \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total \u001b[38;5;241m-\u001b[39m distinct\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[38;5;124;03m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jcols\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2024\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/anaconda3/envs/cemig/lib/python3.9/site-packages/pyspark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column '_keys' does not exist. Did you mean one of the following? [id, suit, tipo, target, petal_length, petal_width, sepal_length, sepal_width];\n'Project ['_keys]\n+- LogicalRDD [sepal_length#0, sepal_width#1, petal_length#2, petal_width#3, target#4L, tipo#5, suit#6, id#7L], false\n"
     ]
    }
   ],
   "source": [
    "handler_measures.run(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itau",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
