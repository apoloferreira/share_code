{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "dir_warehouse = f\"{current_dir}/warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:38:45 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "25/01/09 21:38:45 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/apolo/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/apolo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/apolo/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-9a5f6236-9534-4ff5-84e2-69918e260623;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.6.1 in central\n",
      "\tfound org.postgresql#postgresql;42.3.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 134ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.6.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.3.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-9a5f6236-9534-4ff5-84e2-69918e260623\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/4ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:38:46 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:38:46 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergWithSpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.6.1,org.postgresql:postgresql:42.3.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", dir_warehouse) \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ../Iceberg/vendas_iceberg.zip -d ./\n",
    "# !mkdir -p ./warehouse/default/vendas_iceberg\n",
    "# !cp -r ../Iceberg/vendas_iceberg/* ./warehouse/default/vendas_iceberg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclui se existir\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas\")\n",
    "\n",
    "# Cria a tabela Vendas\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE hadoop_catalog.default.vendas (\n",
    "    id INT,\n",
    "    produto STRING,\n",
    "    quantidade INT,\n",
    "    preco DOUBLE,\n",
    "    data_venda DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Inserir dados fracionados para criar vários arquivos\n",
    "\n",
    "def inserir_dados(pequeno_lote):\n",
    "    df = spark.createDataFrame(pequeno_lote, [\"id\", \"produto\", \"quantidade\", \"preco\", \"data_venda\"])\n",
    "    df = df.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "    df.writeTo(\"hadoop_catalog.default.vendas\").append()\n",
    "\n",
    "\n",
    "# 10 Lotes de Dados\n",
    "for i in range(1, 11):\n",
    "    data = [(i, f\"Produto {i}\", i * 2, i * 10.0, f\"2024-11-{i:02d}\")]\n",
    "    inserir_dados(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes da compactação:\n",
      "+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|registros|arquivo                                                                                                                                                                                                |\n",
      "+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-79-2d80149b-1984-4028-912b-dd4b9671a311-0-00001.parquet |\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-47-36551641-8f88-43cc-9f8c-b264c632554c-0-00001.parquet |\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-95-fd9bbc95-6eac-4129-aeb3-12e30de6094d-0-00001.parquet |\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-111-b50d98bc-3851-4803-be7e-bf64ebbe1513-0-00001.parquet|\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-63-a062f6d3-4a7e-44a9-82ce-89c778afecb3-0-00001.parquet |\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-15-ec505d74-613c-480b-b625-9c33db111fa0-0-00001.parquet |\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-159-2e07c93d-9f6e-4737-ae73-fa74609c0b54-0-00001.parquet|\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-143-b3305402-a547-4b58-a02e-ed9db375ae93-0-00001.parquet|\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-127-1b07c818-8f7c-488b-b723-d3c9c212538e-0-00001.parquet|\n",
      "|1        |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-31-f82c46a8-0c78-425a-bfb0-48408a500e32-0-00001.parquet |\n",
      "+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Número total de arquivos: 10\n"
     ]
    }
   ],
   "source": [
    "# Contar arquivos e registros\n",
    "print(\"Antes da compactação:\")\n",
    "df_files_before = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS registros,\n",
    "    input_file_name() AS arquivo\n",
    "FROM hadoop_catalog.default.vendas\n",
    "GROUP BY input_file_name()\n",
    "\"\"\")\n",
    "df_files_before.show(100, truncate=False)\n",
    "\n",
    "\n",
    "num_arquivos_antes = df_files_before.count()\n",
    "print(f\"Número total de arquivos: {num_arquivos_antes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.8ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.8"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[rewritten_data_files_count: int, added_data_files_count: int, rewritten_bytes_count: bigint]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tamanho máximo de registros por arquivo\n",
    "spark.conf.set(\"spark.sql.files.maxRecordsPerFile\", 1000)\n",
    "\n",
    "# Compactação com proc 'rewrite_data_files'\n",
    "spark.sql(\"\"\"\n",
    "CALL hadoop_catalog.system.rewrite_data_files(\n",
    "    table => 'default.vendas'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Após a compactação:\n",
      "+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|registros|arquivo                                                                                                                                                                                                |\n",
      "+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|10       |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00000-164-fc415516-8d39-4154-9c16-bb1cde2ce0c7-0-00001.parquet|\n",
      "+---------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n",
      "Número total de arquivos após a compactação: 1\n"
     ]
    }
   ],
   "source": [
    "# Contar arquivos e registros\n",
    "print(\"Após a compactação:\")\n",
    "df_files_after = spark.sql(\"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS registros,\n",
    "    input_file_name() AS arquivo\n",
    "FROM hadoop_catalog.default.vendas\n",
    "GROUP BY input_file_name()\n",
    "\"\"\")\n",
    "df_files_after.show(truncate=False)\n",
    "\n",
    "num_arquivos_depois = df_files_after.count()\n",
    "print(f\"Número total de arquivos após a compactação: {num_arquivos_depois}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[deleted_data_files_count: bigint, deleted_position_delete_files_count: bigint, deleted_equality_delete_files_count: bigint, deleted_manifest_files_count: bigint, deleted_manifest_lists_count: bigint, deleted_statistics_files_count: bigint]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# definindo período de retenação\n",
    "spark.sql(\"\"\"\n",
    "CALL hadoop_catalog.system.expire_snapshots(\n",
    "    table => 'default.vendas',\n",
    "    retain_last => 1\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
