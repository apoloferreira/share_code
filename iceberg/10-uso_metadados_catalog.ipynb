{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "dir_warehouse = f\"{current_dir}/warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:50:18 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "25/01/09 21:50:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/apolo/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/apolo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/apolo/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-249f2e58-debb-4d44-aaaa-87adec112767;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.6.1 in central\n",
      "\tfound org.postgresql#postgresql;42.3.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      ":: resolution report :: resolve 89ms :: artifacts dl 4ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.6.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.3.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   0   |   0   |   0   ||   3   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-249f2e58-debb-4d44-aaaa-87adec112767\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 3 already retrieved (0kB/3ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:50:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:50:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergWithSpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.6.1,org.postgresql:postgresql:42.3.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", dir_warehouse) \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ../Iceberg/vendas_iceberg.zip -d ./\n",
    "# !mkdir -p ./warehouse/default/vendas_iceberg\n",
    "# !cp -r ../Iceberg/vendas_iceberg/* ./warehouse/default/vendas_iceberg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando Vendas\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE hadoop_catalog.default.vendas (\n",
    "    id INT,\n",
    "    produto STRING,\n",
    "    quantidade INT,\n",
    "    preco DOUBLE,\n",
    "    data_venda DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Incluindo dados\n",
    "data = [\n",
    "    (1, \"Produto A\", 10, 15.5, \"2024-11-01\"),\n",
    "    (2, \"Produto B\", 5, 22.0, \"2024-11-02\"),\n",
    "    (3, \"Produto C\", 8, 30.0, \"2024-11-03\")\n",
    "]\n",
    "columns = [\"id\", \"produto\", \"quantidade\", \"preco\", \"data_venda\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df.writeTo(\"hadoop_catalog.default.vendas\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2024-11-01|\n",
      "|  2|Produto B|         5| 22.0|2024-11-02|\n",
      "|  3|Produto C|         8| 30.0|2024-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vizualizando Dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de Metadados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas no catálogo:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Visualizando schemas do catálogo\n",
    "print(\"Schemas no catálogo:\")\n",
    "spark.sql(\"\"\"SHOW SCHEMAS IN hadoop_catalog\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas no schema default:\n",
      "+---------+--------------------+-----------+\n",
      "|namespace|           tableName|isTemporary|\n",
      "+---------+--------------------+-----------+\n",
      "|  default|      vendas_iceberg|      false|\n",
      "|  default|  vendas_partitioned|      false|\n",
      "|  default|    vendas_versioned|      false|\n",
      "|  default|              vendas|      false|\n",
      "|  default|clientes_partitioned|      false|\n",
      "+---------+--------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Visualizando tabelas do schema default\n",
    "print(\"Tabelas no schema default:\")\n",
    "spark.sql(\"\"\"SHOW TABLES IN hadoop_catalog.default\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema da tabela vendas:\n",
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|             id|      int|       |\n",
      "|        produto|   string|       |\n",
      "|     quantidade|      int|       |\n",
      "|          preco|   double|       |\n",
      "|     data_venda|     date|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 3. Schema da tabela vendas\n",
    "print(\"Schema da tabela vendas:\")\n",
    "spark.sql(\"\"\"DESCRIBE TABLE hadoop_catalog.default.vendas\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Propriedades da tabela vendas:\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|col_name                    |data_type                                                                                                                          |comment|\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "|id                          |int                                                                                                                                |       |\n",
      "|produto                     |string                                                                                                                             |       |\n",
      "|quantidade                  |int                                                                                                                                |       |\n",
      "|preco                       |double                                                                                                                             |       |\n",
      "|data_venda                  |date                                                                                                                               |       |\n",
      "|                            |                                                                                                                                   |       |\n",
      "|# Partitioning              |                                                                                                                                   |       |\n",
      "|Not partitioned             |                                                                                                                                   |       |\n",
      "|                            |                                                                                                                                   |       |\n",
      "|# Metadata Columns          |                                                                                                                                   |       |\n",
      "|_spec_id                    |int                                                                                                                                |       |\n",
      "|_partition                  |struct<>                                                                                                                           |       |\n",
      "|_file                       |string                                                                                                                             |       |\n",
      "|_pos                        |bigint                                                                                                                             |       |\n",
      "|_deleted                    |boolean                                                                                                                            |       |\n",
      "|                            |                                                                                                                                   |       |\n",
      "|# Detailed Table Information|                                                                                                                                   |       |\n",
      "|Name                        |hadoop_catalog.default.vendas                                                                                                      |       |\n",
      "|Location                    |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas|       |\n",
      "|Provider                    |iceberg                                                                                                                            |       |\n",
      "+----------------------------+-----------------------------------------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Propriedades da tabela 'vendas'\n",
    "print(\"Propriedades da tabela vendas:\")\n",
    "spark.sql(\"\"\"DESCRIBE EXTENDED hadoop_catalog.default.vendas\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 5. Listando tabelas de metadados para vendas\n",
    "# metadata_tables = [\"snapshots\", \"manifests\", \"partitions\", \"files\", \"history\", \"refs\"]\n",
    "\n",
    "# for table in metadata_tables:\n",
    "#     print(f\"\\nMetadados da tabela '{table}':\")\n",
    "#     spark.sql(f\"SELECT * FROM hadoop_catalog.default.vendas.{table}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|committed_at           |snapshot_id        |parent_id|operation|manifest_list                                                                                                                                                                                                    |summary                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "+-----------------------+-------------------+---------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|2025-01-09 21:50:27.386|7874342425608235472|null     |append   |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/metadata/snap-7874342425608235472-1-94138fff-be9b-4abf-893b-f4541d584278.avro|{spark.app.id -> local-1736470220168, added-data-files -> 3, added-records -> 3, added-files-size -> 4274, changed-partition-count -> 1, total-records -> 3, total-files-size -> 4274, total-data-files -> 3, total-delete-files -> 0, total-position-deletes -> 0, total-equality-deletes -> 0, engine-version -> 3.3.0, app-id -> local-1736470220168, engine-name -> spark, iceberg-version -> Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)}|\n",
      "+-----------------------+-------------------+---------+---------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.snapshots\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|content|path                                                                                                                                                                                     |length|partition_spec_id|added_snapshot_id  |added_data_files_count|existing_data_files_count|deleted_data_files_count|added_delete_files_count|existing_delete_files_count|deleted_delete_files_count|partition_summaries|\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "|0      |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/metadata/94138fff-be9b-4abf-893b-f4541d584278-m0.avro|7035  |0                |7874342425608235472|3                     |0                        |0                       |0                       |0                          |0                         |[]                 |\n",
      "+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------+-----------------+-------------------+----------------------+-------------------------+------------------------+------------------------+---------------------------+--------------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.manifests\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|record_count|file_count|total_data_file_size_in_bytes|position_delete_record_count|position_delete_file_count|equality_delete_record_count|equality_delete_file_count|last_updated_at        |last_updated_snapshot_id|\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "|3           |3         |4274                         |0                           |0                         |0                           |0                         |2025-01-09 21:50:27.386|7874342425608235472     |\n",
      "+------------+----------+-----------------------------+----------------------------+--------------------------+----------------------------+--------------------------+-----------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.partitions\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+------------+------------------+---------------------------------------------+----------------------------------------+----------------------------------------+----------------+-----------------------------------------------------------------+-----------------------------------------------------------------+------------+-------------+------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|content|file_path                                                                                                                                                                                             |file_format|spec_id|record_count|file_size_in_bytes|column_sizes                                 |value_counts                            |null_value_counts                       |nan_value_counts|lower_bounds                                                     |upper_bounds                                                     |key_metadata|split_offsets|equality_ids|sort_order_id|readable_metrics                                                                                                                                               |\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+------------+------------------+---------------------------------------------+----------------------------------------+----------------------------------------+----------------+-----------------------------------------------------------------+-----------------------------------------------------------------+------------+-------------+------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|0      |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00005-5-798782ee-d643-4a78-8d56-f686266e32ea-0-00001.parquet |PARQUET    |0      |1           |1424              |{1 -> 42, 2 -> 50, 3 -> 42, 4 -> 46, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|{4 -> 0}        |{1 -> \u0001\u0000\u0000\u0000, 2 -> Produto A, 3 -> \\n\u0000\u0000\u0000, 4 -> \u0000\u0000\u0000\u0000\u0000\u0000/@, 5 -> <N\u0000\u0000}|{1 -> \u0001\u0000\u0000\u0000, 2 -> Produto A, 3 -> \\n\u0000\u0000\u0000, 4 -> \u0000\u0000\u0000\u0000\u0000\u0000/@, 5 -> <N\u0000\u0000}|null        |[4]          |null        |0            |{{42, 1, 0, null, 2024-11-01, 2024-11-01}, {42, 1, 0, null, 1, 1}, {46, 1, 0, 0, 15.5, 15.5}, {50, 1, 0, null, Produto A, Produto A}, {42, 1, 0, null, 10, 10}}|\n",
      "|0      |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00010-10-798782ee-d643-4a78-8d56-f686266e32ea-0-00001.parquet|PARQUET    |0      |1           |1425              |{1 -> 42, 2 -> 51, 3 -> 42, 4 -> 46, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|{4 -> 0}        |{1 -> \u0002\u0000\u0000\u0000, 2 -> Produto B, 3 -> \u0005\u0000\u0000\u0000, 4 -> \u0000\u0000\u0000\u0000\u0000\u00006@, 5 -> =N\u0000\u0000} |{1 -> \u0002\u0000\u0000\u0000, 2 -> Produto B, 3 -> \u0005\u0000\u0000\u0000, 4 -> \u0000\u0000\u0000\u0000\u0000\u00006@, 5 -> =N\u0000\u0000} |null        |[4]          |null        |0            |{{42, 1, 0, null, 2024-11-02, 2024-11-02}, {42, 1, 0, null, 2, 2}, {46, 1, 0, 0, 22.0, 22.0}, {51, 1, 0, null, Produto B, Produto B}, {42, 1, 0, null, 5, 5}}  |\n",
      "|0      |/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse/default/vendas/data/00015-15-798782ee-d643-4a78-8d56-f686266e32ea-0-00001.parquet|PARQUET    |0      |1           |1425              |{1 -> 42, 2 -> 51, 3 -> 42, 4 -> 46, 5 -> 42}|{1 -> 1, 2 -> 1, 3 -> 1, 4 -> 1, 5 -> 1}|{1 -> 0, 2 -> 0, 3 -> 0, 4 -> 0, 5 -> 0}|{4 -> 0}        |{1 -> \u0003\u0000\u0000\u0000, 2 -> Produto C, 3 -> \\b\u0000\u0000\u0000, 4 -> \u0000\u0000\u0000\u0000\u0000\u0000>@, 5 -> >N\u0000\u0000}|{1 -> \u0003\u0000\u0000\u0000, 2 -> Produto C, 3 -> \\b\u0000\u0000\u0000, 4 -> \u0000\u0000\u0000\u0000\u0000\u0000>@, 5 -> >N\u0000\u0000}|null        |[4]          |null        |0            |{{42, 1, 0, null, 2024-11-03, 2024-11-03}, {42, 1, 0, null, 3, 3}, {46, 1, 0, 0, 30.0, 30.0}, {51, 1, 0, null, Produto C, Produto C}, {42, 1, 0, null, 8, 8}}  |\n",
      "+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+-------+------------+------------------+---------------------------------------------+----------------------------------------+----------------------------------------+----------------+-----------------------------------------------------------------+-----------------------------------------------------------------+------------+-------------+------------+-------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.files\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|made_current_at        |snapshot_id        |parent_id|is_current_ancestor|\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "|2025-01-09 21:50:27.386|7874342425608235472|null     |true               |\n",
      "+-----------------------+-------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.history\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|name|type  |snapshot_id        |max_reference_age_in_ms|min_snapshots_to_keep|max_snapshot_age_in_ms|\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "|main|BRANCH|7874342425608235472|null                   |null                 |null                  |\n",
      "+----+------+-------------------+-----------------------+---------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.refs\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catálogos:\n",
      "+--------------+\n",
      "|       catalog|\n",
      "+--------------+\n",
      "|hadoop_catalog|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Catálogos disponíveis\n",
    "print(\"Catálogos:\")\n",
    "spark.sql(\"\"\"SHOW CATALOGS\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visualizando hadoop_catalog:\n",
      "spark.sql.catalog.hadoop_catalog.warehouse: /home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/warehouse\n",
      "spark.sql.catalog.hadoop_catalog: org.apache.iceberg.spark.SparkCatalog\n",
      "spark.sql.catalog.hadoop_catalog.type: hadoop\n"
     ]
    }
   ],
   "source": [
    "# 7. Visualizando catálogo hadoop_catalog\n",
    "print(\"Visualizando hadoop_catalog:\")\n",
    "catalog_conf = spark.sparkContext.getConf().getAll()\n",
    "\n",
    "for key, value in catalog_conf:\n",
    "    if 'hadoop_catalog' in key:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Novo schema sales\n",
    "spark.sql(\"\"\"CREATE SCHEMA IF NOT EXISTS hadoop_catalog.sales\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schemas no hadoop_catalog após adicionar sales:\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|    sales|\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Listando os schemas\n",
    "print(\"Schemas no hadoop_catalog após adicionar sales:\")\n",
    "\n",
    "spark.sql(\"\"\"SHOW SCHEMAS IN hadoop_catalog\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. Criando tabela no schema sales\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE hadoop_catalog.sales.orders (\n",
    "    order_id INT,\n",
    "    customer_id INT,\n",
    "    amount DOUBLE,\n",
    "    order_date DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tabelas no schema 'sales':\n",
      "+---------+---------+-----------+\n",
      "|namespace|tableName|isTemporary|\n",
      "+---------+---------+-----------+\n",
      "|    sales|   orders|      false|\n",
      "+---------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando tabelas no schema sales\n",
    "print(\"Tabelas no schema 'sales':\")\n",
    "\n",
    "spark.sql(\"SHOW TABLES IN hadoop_catalog.sales\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
