{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "dir_warehouse = f\"{current_dir}/warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/07 21:14:34 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "25/01/07 21:14:34 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/07 21:14:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergWithSpark\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", dir_warehouse) \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Versões e Rollbacks de Tabelas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criação da Tabela\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas_versioned\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE hadoop_catalog.default.vendas_versioned (\n",
    "    id INT,\n",
    "    produto STRING,\n",
    "    quantidade INT,\n",
    "    preco DOUBLE,\n",
    "    data_venda DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Dados\n",
    "data_initial = [\n",
    "    (1, \"Produto A\", 10, 15.5, \"2023-11-01\"),\n",
    "    (2, \"Produto B\", 5, 22.0, \"2023-11-02\"),\n",
    "    (3, \"Produto C\", 8, 30.0, \"2023-11-03\")\n",
    "]\n",
    "columns = [\"id\", \"produto\", \"quantidade\", \"preco\", \"data_venda\"]\n",
    "\n",
    "df_initial = spark.createDataFrame(data_initial, columns)\n",
    "df_initial = df_initial.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df_initial.writeTo(\"hadoop_catalog.default.vendas_versioned\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualização\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos um novo snapshot inserindo mais dados\n",
    "data_additional = [\n",
    "    (4, \"Produto D\", 12, 25.0, \"2023-11-04\"),\n",
    "    (5, \"Produto E\", 7, 18.5, \"2023-11-05\")\n",
    "]\n",
    "df_additional = spark.createDataFrame(data_additional, columns)\n",
    "df_additional = df_additional.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "df_additional.writeTo(\"hadoop_catalog.default.vendas_versioned\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualizamos preço\n",
    "spark.sql(\"\"\"\n",
    "UPDATE hadoop_catalog.default.vendas_versioned\n",
    "SET preco = 16.0\n",
    "WHERE id = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Excluimos registro 2\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM hadoop_catalog.default.vendas_versioned\n",
    "WHERE id = 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 16.0|2023-11-01|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# visualização\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned order by id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|7445524589099476635|2025-01-07 21:14:44.571|append   |\n",
      "|6064031807186397825|2025-01-07 21:15:01.051|append   |\n",
      "|4508017605226503077|2025-01-07 21:15:33.681|overwrite|\n",
      "|3192364198481442317|2025-01-07 21:15:33.847|delete   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizamos os snapshots\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned.snapshots\")\n",
    "snapshots_df.select(\"snapshot_id\", \"committed_at\", \"operation\").show(truncate=False)\n",
    "# snapshots_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snapshot ID: 6064031807186397825\n"
     ]
    }
   ],
   "source": [
    "# Obtemos o ID do snapshot para fazer rollback, indíce 1\n",
    "snapshot_ids = snapshots_df.select(\"snapshot_id\").orderBy(\"committed_at\").collect()\n",
    "rollback_snapshot_id = snapshot_ids[1].snapshot_id\n",
    "print(f\"Snapshot ID: {rollback_snapshot_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes do Rollback:\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 16.0|2023-11-01|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Antes do Rollback:\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned order by id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.8ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.8"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[previous_snapshot_id: bigint, current_snapshot_id: bigint]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rollback usado rollback_to_snapshot procedure\n",
    "spark.sql(f\"\"\"\n",
    "CALL hadoop_catalog.system.rollback_to_snapshot(\n",
    "    table => 'hadoop_catalog.default.vendas_versioned',\n",
    "    snapshot_id => {rollback_snapshot_id}\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dados depois do Rollback:\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dados depois do Rollback:\")\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas_versioned order by id\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
