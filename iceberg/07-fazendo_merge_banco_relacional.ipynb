{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "dir_warehouse = f\"{current_dir}/warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/07 21:25:16 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "25/01/07 21:25:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/07 21:25:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergWithSpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.6.1,org.postgresql:postgresql:42.3.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", dir_warehouse) \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ../Iceberg/vendas_iceberg.zip -d ./\n",
    "# !mkdir -p ./warehouse/default/vendas_iceberg\n",
    "# !cp -r ../Iceberg/vendas_iceberg/* ./warehouse/default/vendas_iceberg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/spark-warehouse')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(dbName=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criamos a tabela vendas\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas_iceberg\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas_iceberg (\n",
    "    id_vendas INT,\n",
    "    id_veiculos INT,\n",
    "    id_concessionarias INT,\n",
    "    id_vendedores INT,\n",
    "    id_clientes INT,\n",
    "    valor_pago DOUBLE,\n",
    "    data_venda TIMESTAMP,\n",
    "    data_inclusao TIMESTAMP,\n",
    "    data_atualizacao TIMESTAMP\n",
    ")\n",
    "USING iceberg\n",
    "PARTITIONED BY (days(data_venda))\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credenciais do PostgreSQL\n",
    "jdbc_hostname = \"159.223.187.110\"\n",
    "jdbc_port = 5432\n",
    "jdbc_database = \"novadrive\"\n",
    "jdbc_username = \"etlreadonly\"\n",
    "jdbc_password = \"novadrive376A@\"\n",
    "\n",
    "# URL JDBC de conexão\n",
    "jdbc_url = f\"jdbc:postgresql://{jdbc_hostname}:{jdbc_port}/{jdbc_database}\"\n",
    "\n",
    "# Propriedades de conexão JDBC\n",
    "connection_properties = {\n",
    "    \"user\": jdbc_username,\n",
    "    \"password\": jdbc_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_postgres_data(last_run_timestamp):\n",
    "    query = f\"\"\"(\n",
    "    SELECT *\n",
    "    FROM vendas\n",
    "    WHERE data_inclusao > '{last_run_timestamp}' OR data_atualizacao > '{last_run_timestamp}'\n",
    "    ) AS vendas_subquery\n",
    "    \"\"\"\n",
    "    df = spark.read.jdbc(\n",
    "        url=jdbc_url,\n",
    "        table=query,\n",
    "        properties=connection_properties\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data seis meses atrás: 2024-07-07 21:57:33\n"
     ]
    }
   ],
   "source": [
    "# Timestamp apenas para a primeira execução\n",
    "\n",
    "# Data atual\n",
    "current_date = datetime.now()\n",
    "\n",
    "# Data de seis meses atrás\n",
    "last_run_timestamp = current_date - relativedelta(months=6)\n",
    "\n",
    "# Formatar como string no formato desejado\n",
    "last_run_timestamp_str = last_run_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(\"Data seis meses atrás:\", last_run_timestamp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ler dados do PostgreSQL\n",
    "df_postgres = read_postgres_data(last_run_timestamp)\n",
    "\n",
    "# Exibir os dados lidos\n",
    "df_postgres.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma visão temporária\n",
    "df_postgres.createOrReplaceTempView(\"vendas_updates\")\n",
    "\n",
    "# Executar o MERGE INTO\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO hadoop_catalog.default.vendas_iceberg AS target\n",
    "USING vendas_updates AS source\n",
    "ON target.id_vendas = source.id_vendas\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET *\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = spark.sql(\"SELECT COUNT(*) FROM hadoop_catalog.default.vendas_iceberg\").collect()[0][0]\n",
    "print(f\"Total de linhas na tabela vendas_iceberg: {row_count}\")\n",
    "# 26214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atualizar o timestamp da última execução\n",
    "last_run_timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(f\"Processo concluído. Novo timestamp da última execução: {last_run_timestamp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
