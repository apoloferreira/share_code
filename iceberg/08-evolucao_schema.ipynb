{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "dir_warehouse = f\"{current_dir}/warehouse\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:24:17 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "25/01/09 21:24:17 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/apolo/anaconda3/envs/pyspark/lib/python3.10/site-packages/pyspark/jars/ivy-2.5.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/apolo/.ivy2/cache\n",
      "The jars for the packages stored in: /home/apolo/.ivy2/jars\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.3_2.12 added as a dependency\n",
      "org.postgresql#postgresql added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-92254366-de4a-42d7-922e-d30f11af58ca;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.6.1 in central\n",
      "\tfound org.postgresql#postgresql;42.3.1 in central\n",
      "\tfound org.checkerframework#checker-qual;3.5.0 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-3.3_2.12/1.6.1/iceberg-spark-runtime-3.3_2.12-1.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.6.1!iceberg-spark-runtime-3.3_2.12.jar (4175ms)\n",
      "downloading https://repo1.maven.org/maven2/org/postgresql/postgresql/42.3.1/postgresql-42.3.1.jar ...\n",
      "\t[SUCCESSFUL ] org.postgresql#postgresql;42.3.1!postgresql.jar (281ms)\n",
      "downloading https://repo1.maven.org/maven2/org/checkerframework/checker-qual/3.5.0/checker-qual-3.5.0.jar ...\n",
      "\t[SUCCESSFUL ] org.checkerframework#checker-qual;3.5.0!checker-qual.jar (270ms)\n",
      ":: resolution report :: resolve 1600ms :: artifacts dl 4733ms\n",
      "\t:: modules in use:\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.3_2.12;1.6.1 from central in [default]\n",
      "\torg.checkerframework#checker-qual;3.5.0 from central in [default]\n",
      "\torg.postgresql#postgresql;42.3.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   3   |   3   |   3   |   0   ||   3   |   3   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-92254366-de4a-42d7-922e-d30f11af58ca\n",
      "\tconfs: [default]\n",
      "\t3 artifacts copied, 0 already retrieved (41910kB/31ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/09 21:24:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergWithSpark\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.iceberg:iceberg-spark-runtime-3.3_2.12:1.6.1,org.postgresql:postgresql:42.3.1\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", dir_warehouse) \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip ../Iceberg/vendas_iceberg.zip -d ./\n",
    "# !mkdir -p ./warehouse/default/vendas_iceberg\n",
    "# !cp -r ../Iceberg/vendas_iceberg/* ./warehouse/default/vendas_iceberg/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='default database', locationUri='file:/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo/spark-warehouse')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables(dbName=\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclui a tabela se existir\n",
    "spark.sql(\"DROP TABLE IF EXISTS hadoop_catalog.default.vendas\")\n",
    "\n",
    "# Cria a tabela Vendas no catalogo, usando Iceberg\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE hadoop_catalog.default.vendas (\n",
    "    id INT,\n",
    "    produto STRING,\n",
    "    quantidade INT,\n",
    "    preco DOUBLE,\n",
    "    data_venda DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Incluindo dados na tabela vendas\n",
    "data = [\n",
    "    (1, \"Produto A\", 10, 15.5, \"2024-11-01\"),\n",
    "    (2, \"Produto B\", 5, 22.0, \"2024-11-02\"),\n",
    "    (3, \"Produto C\", 8, 30.0, \"2024-11-03\")\n",
    "]\n",
    "columns = [\"id\", \"produto\", \"quantidade\", \"preco\", \"data_venda\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df = df.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Gravamos os dados na tabela vendas\n",
    "df.writeTo(\"hadoop_catalog.default.vendas\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2024-11-01|\n",
      "|  2|Produto B|         5| 22.0|2024-11-02|\n",
      "|  3|Produto C|         8| 30.0|2024-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificamos os dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolução do Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nova coluna desconto\n",
    "spark.sql(\"\"\"\n",
    "ALTER TABLE hadoop_catalog.default.vendas\n",
    "ADD COLUMN desconto DOUBLE\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------+-------+\n",
      "|       col_name|data_type|comment|\n",
      "+---------------+---------+-------+\n",
      "|             id|      int|       |\n",
      "|        produto|   string|       |\n",
      "|     quantidade|      int|       |\n",
      "|          preco|   double|       |\n",
      "|     data_venda|     date|       |\n",
      "|       desconto|   double|       |\n",
      "|               |         |       |\n",
      "| # Partitioning|         |       |\n",
      "|Not partitioned|         |       |\n",
      "+---------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema Atualizado\n",
    "spark.sql(\"DESCRIBE hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserindo dados com a nova coluna\n",
    "data_new = [\n",
    "    (4, \"Produto D\", 12, 25.0, \"2024-11-04\", 2.5),\n",
    "    (5, \"Produto E\", 7, 18.0, \"2024-11-05\", 1.0)\n",
    "]\n",
    "columns_new = [\"id\", \"produto\", \"quantidade\", \"preco\", \"data_venda\", \"desconto\"]\n",
    "df_new = spark.createDataFrame(data_new, columns_new)\n",
    "df_new = df_new.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "\n",
    "# Gravando os dados\n",
    "df_new.writeTo(\"hadoop_catalog.default.vendas\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+--------+\n",
      "| id|  produto|quantidade|preco|data_venda|desconto|\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "|  4|Produto D|        12| 25.0|2024-11-04|     2.5|\n",
      "|  1|Produto A|        10| 15.5|2024-11-01|    null|\n",
      "|  5|Produto E|         7| 18.0|2024-11-05|     1.0|\n",
      "|  2|Produto B|         5| 22.0|2024-11-02|    null|\n",
      "|  3|Produto C|         8| 30.0|2024-11-03|    null|\n",
      "+---+---------+----------+-----+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultando\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['committed_at', 'snapshot_id', 'parent_id', 'operation', 'manifest_list', 'summary']\n"
     ]
    }
   ],
   "source": [
    "# mostra snapshots\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.snapshots order by committed_at asc\")\n",
    "print(snapshots_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-------------------+---------+\n",
      "|committed_at           |snapshot_id        |operation|\n",
      "+-----------------------+-------------------+---------+\n",
      "|2025-01-09 21:24:44.304|4794066217950379325|append   |\n",
      "|2025-01-09 21:25:31.192|8905770237618997467|append   |\n",
      "+-----------------------+-------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snapshots_df.select(\"committed_at\", \"snapshot_id\", \"operation\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8905770237618997467\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'engine-version': '3.3.0',\n",
       " 'added-data-files': '2',\n",
       " 'total-equality-deletes': '0',\n",
       " 'app-id': 'local-1736468665595',\n",
       " 'added-records': '2',\n",
       " 'total-records': '5',\n",
       " 'spark.app.id': 'local-1736468665595',\n",
       " 'changed-partition-count': '1',\n",
       " 'engine-name': 'spark',\n",
       " 'total-position-deletes': '0',\n",
       " 'added-files-size': '3380',\n",
       " 'total-delete-files': '0',\n",
       " 'iceberg-version': 'Apache Iceberg 1.6.1 (commit 8e9d59d299be42b0bca9461457cd1e95dbaad086)',\n",
       " 'total-files-size': '7654',\n",
       " 'total-data-files': '5'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record = snapshots_df.select(\"snapshot_id\", \"summary\").collect()\n",
    "\n",
    "idx = 1\n",
    "print(record[idx].snapshot_id)\n",
    "record[idx].summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
