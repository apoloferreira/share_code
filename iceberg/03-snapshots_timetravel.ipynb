{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/apolo/Dropbox/programacao/Udemy/2024/engenharia_de_dados_com_apache_iceberg_e_spark/00-scripts_apolo\n"
     ]
    }
   ],
   "source": [
    "current_dir = os.getcwd()\n",
    "print(current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_warehouse = f\"{current_dir}/warehouse\"\n",
    "dir_jars = f\"{current_dir}/spark-3.3.0-bin-hadoop3/jars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/06 21:21:19 WARN Utils: Your hostname, dell resolves to a loopback address: 127.0.1.1; using 192.168.15.6 instead (on interface wlp0s20f3)\n",
      "25/01/06 21:21:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/01/06 21:21:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"IcebergWithSpark\") \\\n",
    "    .config(\"spark.sql.extensions\", \"org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog\", \"org.apache.iceberg.spark.SparkCatalog\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.type\", \"hadoop\") \\\n",
    "    .config(\"spark.sql.catalog.hadoop_catalog.warehouse\", dir_warehouse) \\\n",
    "    .config(\"spark.sql.default.catalog\", \"hadoop_catalog\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criamos a tabela vendas usando Iceberg\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hadoop_catalog.default.vendas (\n",
    "    id INT,\n",
    "    produto STRING,\n",
    "    quantidade INT,\n",
    "    preco DOUBLE,\n",
    "    data_venda DATE\n",
    ")\n",
    "USING iceberg\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados Iniciais\n",
    "data_initial = [\n",
    "    (1, \"Produto A\", 10, 15.5, \"2023-11-01\"),\n",
    "    (2, \"Produto B\", 5, 22.0, \"2023-11-02\"),\n",
    "    (3, \"Produto C\", 8, 30.0, \"2023-11-03\")\n",
    "]\n",
    "columns = [\"id\", \"produto\", \"quantidade\", \"preco\", \"data_venda\"]\n",
    "\n",
    "df_initial = spark.createDataFrame(data_initial, columns)\n",
    "df_initial = df_initial.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Gravamos os dados\n",
    "df_initial.writeTo(\"hadoop_catalog.default.vendas\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualização dos dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5502038290185584988|2025-01-06 21:37:09.735|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lista de snapshots atuais da tabela vendas\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.snapshots\")\n",
    "\n",
    "snapshots_df.select(\"snapshot_id\", \"committed_at\", \"operation\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incluimos mais dados\n",
    "data_additional = [\n",
    "    (4, \"Produto D\", 12, 25.0, \"2023-11-04\"),\n",
    "    (5, \"Produto E\", 7, 18.5, \"2023-11-05\")\n",
    "]\n",
    "df_additional = spark.createDataFrame(data_additional, columns)\n",
    "df_additional = df_additional.withColumn(\"data_venda\", F.to_date(F.col(\"data_venda\"), \"yyyy-MM-dd\"))\n",
    "df_additional.writeTo(\"hadoop_catalog.default.vendas\").append()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualização dos dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas order by id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5502038290185584988|2025-01-06 21:37:09.735|append   |\n",
      "|3087817526478000336|2025-01-06 21:40:23.168|append   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List os snapshots novamente\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.snapshots\")\n",
    "snapshots_df.select(\"snapshot_id\", \"committed_at\", \"operation\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atualização de dados\n",
    "spark.sql(\"\"\"\n",
    "UPDATE hadoop_catalog.default.vendas\n",
    "SET preco = 16.0\n",
    "WHERE id = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 16.0|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualização dos dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas  order by id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5502038290185584988|2025-01-06 21:37:09.735|append   |\n",
      "|3087817526478000336|2025-01-06 21:40:23.168|append   |\n",
      "|686919947402139456 |2025-01-06 21:42:25.739|overwrite|\n",
      "|2392243818373974793|2025-01-06 21:43:03.196|delete   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List os snapshots novamente\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.snapshots\")\n",
    "snapshots_df.select(\"snapshot_id\", \"committed_at\", \"operation\").orderBy(\"committed_at\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Excluímos dados\n",
    "spark.sql(\"\"\"\n",
    "DELETE FROM hadoop_catalog.default.vendas\n",
    "WHERE id = 2\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 16.0|2023-11-01|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualização dos dados\n",
    "spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas order by id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|5502038290185584988|2025-01-06 21:37:09.735|append   |\n",
      "|3087817526478000336|2025-01-06 21:40:23.168|append   |\n",
      "|686919947402139456 |2025-01-06 21:42:25.739|overwrite|\n",
      "|2392243818373974793|2025-01-06 21:43:03.196|delete   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# List os snapshots novamente\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.snapshots\")\n",
    "snapshots_df.select(\"snapshot_id\", \"committed_at\", \"operation\").orderBy(F.col(\"committed_at\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel por ID do snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(snapshot_id=5502038290185584988), Row(snapshot_id=3087817526478000336), Row(snapshot_id=686919947402139456), Row(snapshot_id=2392243818373974793)]\n"
     ]
    }
   ],
   "source": [
    "# lista de snapshost ordenados pela data de commit\n",
    "snapshot_ids = spark.sql(\"\"\"\n",
    "SELECT snapshot_id\n",
    "FROM hadoop_catalog.default.vendas.snapshots\n",
    "ORDER BY committed_at ASC\n",
    "\"\"\").collect()\n",
    "\n",
    "print(snapshot_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at Snapshot ID 5502038290185584988:\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recuperamos o estado no primeiro snapshot\n",
    "first_snapshot_id = snapshot_ids[0].snapshot_id\n",
    "print(f\"Data at Snapshot ID {first_snapshot_id}:\")\n",
    "\n",
    "spark.read \\\n",
    "    .option(\"snapshot-id\", first_snapshot_id) \\\n",
    "    .table(\"hadoop_catalog.default.vendas\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at Snapshot ID 3087817526478000336:\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# recuperamos o estado no segundo snapshot\n",
    "first_snapshot_id = snapshot_ids[1].snapshot_id\n",
    "print(f\"Data at Snapshot ID {first_snapshot_id}:\")\n",
    "\n",
    "spark.read \\\n",
    "    .option(\"snapshot-id\", first_snapshot_id) \\\n",
    "    .table(\"hadoop_catalog.default.vendas\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel por timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(committed_at=datetime.datetime(2025, 1, 6, 21, 37, 9, 735000))\n",
      "Row(committed_at=datetime.datetime(2025, 1, 6, 21, 40, 23, 168000))\n",
      "Row(committed_at=datetime.datetime(2025, 1, 6, 21, 42, 25, 739000))\n",
      "Row(committed_at=datetime.datetime(2025, 1, 6, 21, 43, 3, 196000))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Recupera os timestamps de committed\n",
    "snapshot_timestamps = spark.sql(\"\"\"\n",
    "SELECT committed_at\n",
    "FROM hadoop_catalog.default.vendas.snapshots\n",
    "ORDER BY committed_at ASC\n",
    "\"\"\").collect()\n",
    "\n",
    "[print(item) for item in snapshot_timestamps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-06 21:40:23.168000\n"
     ]
    }
   ],
   "source": [
    "# Verificamos o timestamp do segundo snapshot\n",
    "second_snapshot_timestamp = snapshot_timestamps[1].committed_at\n",
    "print(second_snapshot_timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data as of 2025-01-06 21:40:23.168000:\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convertemos o timestamp em milesegundos para recuperar o segundo snapshot\n",
    "timestamp_ms = int(second_snapshot_timestamp.timestamp() * 1000)\n",
    "print(f\"Data as of {second_snapshot_timestamp}:\")\n",
    "\n",
    "spark.read \\\n",
    "    .option(\"as-of-timestamp\", timestamp_ms) \\\n",
    "    .table(\"hadoop_catalog.default.vendas\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel usando SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data at Snapshot ID 3087817526478000336 using SQL:\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  4|Produto D|        12| 25.0|2023-11-04|\n",
      "|  5|Produto E|         7| 18.5|2023-11-05|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consulta usando ID do snapshot\n",
    "print(f\"Data at Snapshot ID {first_snapshot_id} using SQL:\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT * FROM hadoop_catalog.default.vendas\n",
    "VERSION AS OF {first_snapshot_id}\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-06 21:40:23\n",
      "Data as of '2025-01-06 21:40:23' using SQL:\n",
      "+---+---------+----------+-----+----------+\n",
      "| id|  produto|quantidade|preco|data_venda|\n",
      "+---+---------+----------+-----+----------+\n",
      "|  1|Produto A|        10| 15.5|2023-11-01|\n",
      "|  2|Produto B|         5| 22.0|2023-11-02|\n",
      "|  3|Produto C|         8| 30.0|2023-11-03|\n",
      "+---+---------+----------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Usando sql com timestamp\n",
    "timestamp_str = second_snapshot_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "print(timestamp_str)\n",
    "print(f\"Data as of '{timestamp_str}' using SQL:\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "SELECT * FROM hadoop_catalog.default.vendas\n",
    "TIMESTAMP AS OF '{timestamp_str}'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definindo Expiração de Snapshots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-06 22:04:24\n"
     ]
    }
   ],
   "source": [
    "# Definimos expiração de snapshots para mais de 1 minuto\n",
    "expire_timestamp = datetime.now() - timedelta(minutes=1)\n",
    "expire_timestamp_str = expire_timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(expire_timestamp_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.8ANTLR Tool version 4.9.3 used for code generation does not match the current runtime version 4.8ANTLR Runtime version 4.9.3 used for parser compilation does not match the current runtime version 4.8"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|deleted_data_files_count|deleted_position_delete_files_count|deleted_equality_delete_files_count|deleted_manifest_files_count|deleted_manifest_lists_count|deleted_statistics_files_count|\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "|2                       |0                                  |0                                  |2                           |3                           |0                             |\n",
      "+------------------------+-----------------------------------+-----------------------------------+----------------------------+----------------------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# executa expireSnapshots\n",
    "expiration_result = spark.sql(f\"\"\"\n",
    "CALL hadoop_catalog.system.expire_snapshots(\n",
    "    table => 'default.vendas',\n",
    "    older_than => TIMESTAMP '{expire_timestamp_str}',\n",
    "    retain_last => 1\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "# Exibe os resultaods\n",
    "expiration_result.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-----------------------+---------+\n",
      "|snapshot_id        |committed_at           |operation|\n",
      "+-------------------+-----------------------+---------+\n",
      "|2392243818373974793|2025-01-06 21:43:03.196|delete   |\n",
      "+-------------------+-----------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mostra snapshots ativos\n",
    "snapshots_df = spark.sql(\"SELECT * FROM hadoop_catalog.default.vendas.snapshots\")\n",
    "snapshots_df.select(\"snapshot_id\", \"committed_at\", \"operation\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exclui tabela\n",
    "spark.sql(\"DROP TABLE hadoop_catalog.default.vendas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
